{"cells":[{"cell_type":"markdown","source":["# `method` on zf"],"metadata":{"id":"D8jxVnzsmLsZ"},"id":"D8jxVnzsmLsZ"},{"cell_type":"markdown","source":["# running `method` on zf"],"metadata":{"id":"pARlHy2zwi-s"},"id":"pARlHy2zwi-s"},{"cell_type":"markdown","source":["## imports"],"metadata":{"id":"s2Ooe2JNmPgy"},"id":"s2Ooe2JNmPgy"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r0mZ_bpcmXEY","executionInfo":{"status":"ok","timestamp":1729364584213,"user_tz":240,"elapsed":14273,"user":{"displayName":"Julian Gold","userId":"01597584131251118338"}},"outputId":"b3b7d472-192b-4c5f-d655-33607ba43ab9"},"id":"r0mZ_bpcmXEY","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install scanpy -q"],"metadata":{"id":"b4OOQ3A5ndW8"},"id":"b4OOQ3A5ndW8","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import scanpy as sc\n","import numpy as np\n","import torch\n","import sys\n","from scipy.spatial.distance import cdist\n","from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n","import sys\n","import importlib"],"metadata":{"id":"gccl_6ynmTLf"},"id":"gccl_6ynmTLf","execution_count":null,"outputs":[]},{"cell_type":"code","source":["filehandle_wdm = 'drive/Othercomputers/numac/GitHub/WDM/'\n","filehandle_save = 'drive/Othercomputers/numac/GitHub/celltypediscovery/_wdm_save_sc/'\n","filehandle_save_factors = 'drive/Othercomputers/numac/GitHub/celltypediscovery/_wdm_save_sc_factors/'\n","filehandle_zf = 'drive/MyDrive/DX/_data/zebrafish/cleaned_common_pca_sc/'\n","\n","sys.path.insert(0, filehandle_wdm)\n","sys.path.insert(0, filehandle_save)\n","sys.path.insert(0, filehandle_zf)\n","\n","import clustering\n","import util_LR\n","import util_zf\n","import FRLC_LRDist"],"metadata":{"id":"WT0KbcLtmUo7"},"id":"WT0KbcLtmUo7","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## reload"],"metadata":{"id":"_2Fa8npruJBQ"},"id":"_2Fa8npruJBQ"},{"cell_type":"code","source":["importlib.reload(clustering)\n","importlib.reload(util_LR)\n","importlib.reload(util_zf)\n","importlib.reload(FRLC_LRDist)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MOa4bIq3p8bd","executionInfo":{"status":"ok","timestamp":1729363431260,"user_tz":240,"elapsed":6,"user":{"displayName":"Julian Gold","userId":"01597584131251118338"}},"outputId":"f2015c7f-0b9f-4b14-b0e0-479586fc91ab"},"id":"MOa4bIq3p8bd","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<module 'FRLC_LRDist' from '/content/drive/Othercomputers/numac/GitHub/WDM/FRLC_LRDist.py'>"]},"metadata":{},"execution_count":89}]},{"cell_type":"markdown","source":["## device"],"metadata":{"id":"n6J1F3NBteCH"},"id":"n6J1F3NBteCH"},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'On device: {device}')\n","dtype = torch.float64"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3_wr5jfunN6d","executionInfo":{"status":"ok","timestamp":1729363431260,"user_tz":240,"elapsed":5,"user":{"displayName":"Julian Gold","userId":"01597584131251118338"}},"outputId":"ed7a3d9d-0eba-4c22-e160-0c0e1a36aa7f"},"id":"3_wr5jfunN6d","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["On device: cuda\n"]}]},{"cell_type":"markdown","source":["## load zf: spatial, annotation hard clusterings as $Q$'s"],"metadata":{"id":"_aV-U7fftfPj"},"id":"_aV-U7fftfPj"},{"cell_type":"code","execution_count":null,"id":"e48e024c-0966-449b-be5d-720071a00023","metadata":{"id":"e48e024c-0966-449b-be5d-720071a00023"},"outputs":[],"source":["# daniocell_dir = '/scratch/gpfs/ph3641/zebrafish/daniocell.h5ad'\n","# adata_daniocell = sc.read_h5ad(daniocell_dir)\n","# stages = np.unique(adata_daniocell.obs['hpf'].values)\n","\n","zf_list = ['zf3', 'zf5', 'zf10', 'zf12', 'zf18', 'zf24']\n","zf_names = zf_list\n","filehandles_zf = [filehandle_zf + 'pair' + str(k) + '/' for k in range(len(zf_list))]\n","N = len(filehandles_zf)\n","\n","spatial_list = []\n","exclude_rows = [None, None, None, None, None, None] # [1099, None, None, 325, None, None]\n","\n","for i in range(len(zf_list)):\n","    file_spatial = filehandles_zf[i] + zf_names[i] +'_umap.npy'\n","    if i == len(zf_list) - 1:\n","        file_spatial = filehandles_zf[i-1] + zf_names[i] +'_umap.npy'\n","    spatial = np.load(file_spatial)\n","    nidx = exclude_rows[i]\n","    if nidx is not None:\n","        spatial = np.concatenate((spatial[:nidx,:], spatial[nidx+1:,:]))\n","    spatial_list.append(spatial)"]},{"cell_type":"code","execution_count":null,"id":"538e86a4-61f4-4e98-a2c0-a1b6a84e5e09","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"538e86a4-61f4-4e98-a2c0-a1b6a84e5e09","executionInfo":{"status":"ok","timestamp":1729363438537,"user_tz":240,"elapsed":5978,"user":{"displayName":"Julian Gold","userId":"01597584131251118338"}},"outputId":"92f650a5-3815-4ed7-87a7-c2dad10bda4d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Slice pair 4, aligning zf18 to zf24\n","Slice pair 3, aligning zf12 to zf18\n","Slice pair 2, aligning zf10 to zf12\n","Slice pair 1, aligning zf5 to zf10\n","Slice pair 0, aligning zf3 to zf5\n"]}],"source":["'''\n","N = len(filehandles_zf)\n","Qs = [None]*(N-1)\n","Rs = [None]*(N-1)\n","adatas_Qs = [None] * (N-1)\n","adatas_Rs = [None] * (N-1)\n","\n","key = 'celltype_new'\n","\n","labels = []\n","\n","for i in range(N-2, -1, -1):\n","\n","    s1_name = zf_names[i]\n","    s2_name = zf_names[i+1]\n","\n","    print(f'Slice pair {i}, aligning {s1_name} to {s2_name}')\n","\n","    adata_pair = sc.read_h5ad(filehandle_zf + f'pair{i}/' + s1_name + '_' + s2_name + '.h5ad')\n","\n","    adata1 = adata_pair[adata_pair.obs['timepoint'] == 1]\n","    # adata1 = adata1[adata1.obs['bin_annotation'] != 'Otic Vesicle']  # Exclude 'Otic Vesicle'\n","\n","    # sc.pp.normalize_total(adata1)\n","    # sc.pp.log1p(adata1)\n","\n","    adata2 = adata_pair[adata_pair.obs['timepoint'] == 2]\n","    # adata2 = adata2[adata2.obs['bin_annotation'] != 'Otic Vesicle']  # Exclude 'Otic Vesicle'\n","    # sc.pp.normalize_total(adata2)\n","    # sc.pp.log1p(adata2)\n","    adatas_Qs[i], adatas_Rs[i] = adata1, adata2\n","\n","    encoder1 = OneHotEncoder(sparse_output=False)\n","    ys_onehot1 = encoder1.fit_transform(adata1.obs[key].values.reshape(-1, 1))\n","    _Q = ys_onehot1 / np.sum(ys_onehot1)\n","\n","    encoder2 = OneHotEncoder(sparse_output=False)\n","    ys_onehot2 = encoder2.fit_transform(adata2.obs[key].values.reshape(-1, 1))\n","    _R = ys_onehot2 / np.sum(ys_onehot2)\n","\n","    labels.append(list(encoder2.categories_[0]))\n","\n","    if i == 0:\n","        labels.append(list(encoder1.categories_[0]))\n","\n","    _Q, _R = torch.from_numpy(_Q).to(device).float(), torch.from_numpy(_R).to(device).float()\n","\n","    # Filter rows\n","    nidx_1, nidx_2 = exclude_rows[i], exclude_rows[i+1]\n","    if nidx_1 is not None:\n","        _Q = torch.cat((_Q[:nidx_1,:], _Q[nidx_1+1:,:]))\n","    if nidx_2 is not None:\n","        _R = torch.cat((_R[:nidx_2,:], _R[nidx_2+1:,:]))\n","\n","    Qs[i], Rs[i] = _Q, _R\n","\n","Qs_gt_tensor = Qs + [Rs[4]]\n","Qs_gt_tensor = [ Q.type(torch.DoubleTensor).to(device) for Q in Qs_gt_tensor]\n","\n","Qs_gt = [Q.cpu().numpy() for Q in Qs_gt_tensor]\n","\n","adatas = adatas_Qs + [adatas_Rs[-1]]\n","'''\n","N = len(filehandles_zf)\n","Qs = [None]*(N-1)\n","Rs = [None]*(N-1)\n","adatas_Qs = [None] * (N-1)\n","adatas_Rs = [None] * (N-1)\n","\n","key = 'celltype_new'\n","\n","labels = []\n","\n","for i in range(N-2, -1, -1):\n","\n","    s1_name = zf_names[i]\n","    s2_name = zf_names[i+1]\n","\n","    print(f'Slice pair {i}, aligning {s1_name} to {s2_name}')\n","\n","    adata_pair = sc.read_h5ad(filehandle_zf + f'pair{i}/' + s1_name + '_' + s2_name + '.h5ad')\n","\n","    adata1 = adata_pair[adata_pair.obs['timepoint'] == 1]\n","    adata2 = adata_pair[adata_pair.obs['timepoint'] == 2]\n","    adatas_Qs[i], adatas_Rs[i] = adata1, adata2\n","\n","    encoder1 = OneHotEncoder(sparse_output=False)\n","    ys_onehot1 = encoder1.fit_transform(adata1.obs[key].values.reshape(-1, 1))\n","    _Q = ys_onehot1 / np.sum(ys_onehot1)\n","\n","    encoder2 = OneHotEncoder(sparse_output=False)\n","    ys_onehot2 = encoder2.fit_transform(adata2.obs[key].values.reshape(-1, 1))\n","    _R = ys_onehot2 / np.sum(ys_onehot2)\n","\n","    labels.append(list(encoder2.categories_[0]))\n","\n","    if i == 0:\n","        labels.append(list(encoder1.categories_[0]))\n","\n","    _Q, _R = torch.from_numpy(_Q).to(device).float(), torch.from_numpy(_R).to(device).float()\n","\n","    # Filter rows\n","    nidx_1, nidx_2 = exclude_rows[i], exclude_rows[i+1]\n","    if nidx_1 is not None:\n","        _Q = torch.cat((_Q[:nidx_1,:], _Q[nidx_1+1:,:]))\n","    if nidx_2 is not None:\n","        _R = torch.cat((_R[:nidx_2,:], _R[nidx_2+1:,:]))\n","\n","    Qs[i], Rs[i] = _Q, _R\n","\n","Qs_gt_tensor = Qs + [Rs[4]]\n","Qs_gt_tensor = [ Q.type(torch.DoubleTensor).to(device) for Q in Qs_gt_tensor]\n","\n","Qs_gt = [Q.cpu().numpy() for Q in Qs_gt_tensor]\n","\n","adatas = adatas_Qs + [adatas_Rs[-1]]\n","\n","# Extract cell type labels directly from adatas\n","ct_labels = [list(adata.obs[key].unique()) for adata in adatas]"]},{"cell_type":"code","source":["for la in ct_labels:\n","    print(len(la))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HG6QUCSjAch3","executionInfo":{"status":"ok","timestamp":1729363438537,"user_tz":240,"elapsed":7,"user":{"displayName":"Julian Gold","userId":"01597584131251118338"}},"outputId":"ebc5db87-c6af-4a40-a40c-388fa2d9b253"},"id":"HG6QUCSjAch3","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2\n","9\n","8\n","12\n","14\n","23\n"]}]},{"cell_type":"markdown","source":["## set ranks"],"metadata":{"id":"JdRFL7cmtVFs"},"id":"JdRFL7cmtVFs"},{"cell_type":"code","source":["print([Q.shape for Q in Qs_gt])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CVMhUobuvkA_","executionInfo":{"status":"ok","timestamp":1729363438537,"user_tz":240,"elapsed":6,"user":{"displayName":"Julian Gold","userId":"01597584131251118338"}},"outputId":"61245356-c9b7-4d85-f225-2feb677a8aac"},"id":"CVMhUobuvkA_","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[(7424, 2), (7136, 9), (8755, 8), (12133, 12), (9903, 14), (14882, 23)]\n"]}]},{"cell_type":"code","source":["ranks = [(2,9),(9,8),(8,12),(12,14),(14, 23)]"],"metadata":{"id":"FQRnE2gwtVzX"},"id":"FQRnE2gwtVzX","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## load features and spatial coords specific to the triple"],"metadata":{"id":"ffVna5X1u8Ec"},"id":"ffVna5X1u8Ec"},{"cell_type":"code","source":["s0_name = zf_names[0]\n","s1_name = zf_names[1]\n","s2_name = zf_names[2]\n","s3_name = zf_names[3]\n","s4_name = zf_names[4]\n","s5_name = zf_names[5]\n","\n","\n","filehandle_pair0 = filehandle_zf + 'pair' + str(0) + '/'\n","filehandle_pair1 = filehandle_zf + 'pair' + str(1) + '/'\n","filehandle_pair2 = filehandle_zf + 'pair' + str(2) + '/'\n","filehandle_pair3 = filehandle_zf + 'pair' + str(3) + '/'\n","filehandle_pair4 = filehandle_zf + 'pair' + str(4) + '/'\n","\n","\n","X0 = np.load(filehandle_pair0 + s0_name + '_feature.npy')\n","X1 = np.load(filehandle_pair1 + s1_name + '_feature.npy')\n","X2 = np.load(filehandle_pair2 + s2_name + '_feature.npy')\n","X3 = np.load(filehandle_pair3 + s3_name + '_feature.npy')\n","X4 = np.load(filehandle_pair4 + s4_name + '_feature.npy')\n","X5 = np.load(filehandle_pair4 + s5_name + '_feature.npy')\n","\n","Xs = [X0, X1, X2, X3, X4, X5]\n","Ss = spatial_list"],"metadata":{"id":"0w0o0t_lfrcT"},"id":"0w0o0t_lfrcT","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## set OT hyperparameters"],"metadata":{"id":"OERTFrT3uhRF"},"id":"OERTFrT3uhRF"},{"cell_type":"code","source":["gamma=40\n","tau_out=1e5\n","tau_in=5e-8\n","alpha = 0.0\n","beta = 0.0\n","\n","max_iter=100\n","min_iter=100"],"metadata":{"id":"iBHClrIPuiFV"},"id":"iBHClrIPuiFV","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## form cost matrices"],"metadata":{"id":"faIcwmVvtWXX"},"id":"faIcwmVvtWXX"},{"cell_type":"code","source":["importlib.reload(util_zf)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-bEcGK4ofhVV","executionInfo":{"status":"ok","timestamp":1729363440258,"user_tz":240,"elapsed":5,"user":{"displayName":"Julian Gold","userId":"01597584131251118338"}},"outputId":"43485d93-54bb-4e85-a0a4-71986895c773"},"id":"-bEcGK4ofhVV","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<module 'util_zf' from '/content/drive/Othercomputers/numac/GitHub/WDM/util_zf.py'>"]},"metadata":{},"execution_count":98}]},{"cell_type":"code","source":["%%time\n","Aiis_np = []\n","\n","truncation = 100\n","time = 50.0\n","epsilon = 1/gamma\n","\n","HDM = False\n","MELD = False\n","\n","if HDM==False:\n","    for i, S in enumerate(Ss):\n","        Aii_np = cdist(S, S)\n","        Aiis_np += [Aii_np]\n","elif HDM==True and MELD==False:\n","    for i, X in enumerate(Xs):\n","        X_HDM = util_zf.HDM_from_XS(Ss[i], Xs[i], truncation, time)\n","        Cii_np = cdist(X_HDM, X_HDM)\n","        Aii_np = np.exp(-Cii_np/epsilon)\n","        Aiis_np += [Aii_np]\n","else:\n","    for i, X in enumerate(Xs):\n","        X_HDM = util_zf.HDM_from_XS(Xs[i], Xs[i], truncation, time, MELD, epsilon)\n","        Aii_np = cdist(X_HDM, X_HDM)\n","        Aiis_np += [Aii_np]\n","\n","\n","Aiis = [torch.from_numpy(Aii).to(device) for Aii in Aiis_np]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vfw-822gvZU5","executionInfo":{"status":"ok","timestamp":1729363444253,"user_tz":240,"elapsed":3998,"user":{"displayName":"Julian Gold","userId":"01597584131251118338"}},"outputId":"be9a0030-88fc-4a85-a132-547c6e53ae1f"},"id":"vfw-822gvZU5","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 3.06 s, sys: 923 ms, total: 3.98 s\n","Wall time: 3.95 s\n"]}]},{"cell_type":"markdown","source":["## factor cost matrices"],"metadata":{"id":"nWWpDJ1mzEbH"},"id":"nWWpDJ1mzEbH"},{"cell_type":"code","execution_count":null,"id":"cf4a9fa0-3cfa-4946-9aa0-a2f4eaead41c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cf4a9fa0-3cfa-4946-9aa0-a2f4eaead41c","outputId":"c748998e-a766-4f82-9683-ba581b2d4338"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading slices zf3 to zf5\n","C done\n","A done\n","B done\n","Loading slices zf5 to zf10\n","C done\n","A done\n","B done\n","Loading slices zf10 to zf12\n","C done\n","A done\n","B done\n","Loading slices zf12 to zf18\n","C done\n"]}],"source":["new_expr_signal=True\n","\n","if new_expr_signal==True:\n","    C_factors_sequence = []\n","    A_factors_sequence = []\n","\n","    for i in range(len(ranks)):\n","\n","        s1_name = zf_names[i]\n","        s2_name = zf_names[i+1]\n","\n","        print(f'Loading slices {s1_name} to {s2_name}')\n","\n","        r1, r2 = ranks[i]\n","        nidx_1, nidx_2 = exclude_rows[i], exclude_rows[i+1]\n","\n","        filehandle_pair = filehandle_zf + 'pair' + str(i) + '/'\n","        save_dir = filehandle_pair\n","        C_matrix = cdist(Xs[i], Xs[i+1])\n","        C_factors12, A_factors11, A_factors22 = util_zf.factor_mats(C_matrix, Aiis[i].cpu().numpy(), Aiis[i+1].cpu().numpy(), \\\n","                                                        device, z=50, c=100, nidx_1=nidx_1, nidx_2=nidx_2)\n","        C_factors_sequence.append(C_factors12)\n","        if i == range(len(ranks))[-1]:\n","            A_factors_sequence.append(A_factors11)\n","            A_factors_sequence.append(A_factors22)\n","        else:\n","            A_factors_sequence.append(A_factors11)\n","\n","\n","    c = max([torch.max(C[0] @ C[1]) for C in C_factors_sequence])\n","    c = max(c, 1/c)\n","    for i in range(len(C_factors_sequence)):\n","        C = C_factors_sequence[i]\n","        C_factors_sequence[i] = (C[0]/c, C[1]/c)\n","else:\n","    pass\n"]},{"cell_type":"markdown","source":["## save factors"],"metadata":{"id":"NcttRWE5xlB4"},"id":"NcttRWE5xlB4"},{"cell_type":"code","source":["import os\n","\n","save_dir = filehandle_save_factors\n","\n","if new_expr_signal == True:\n","    os.makedirs(save_dir, exist_ok=True)\n","    N = len(A_factors_sequence)\n","    if len(C_factors_sequence) != N - 1:\n","        raise ValueError(\"C_factors_sequence must be of length N - 1, where N is the length of A_factors_sequence.\")\n","\n","    for idx in range(N):\n","        A_tuple = A_factors_sequence[idx]\n","        for arr_idx, array in enumerate(A_tuple):\n","            if not isinstance(array, np.ndarray):\n","                raise TypeError(f\"Element at A_factors_sequence[{idx}][{arr_idx}] is not a NumPy array.\")\n","            filename = os.path.join(save_dir, f'A_array_{idx}_{arr_idx}.npy')\n","            try:\n","                np.save(filename, array)\n","                print(f\"Saved {filename}\")\n","            except Exception as e:\n","                print(f\"Failed to save {filename}: {e}\")\n","\n","        if idx < N - 1:\n","            C_tuple = C_factors_sequence[idx]\n","            for arr_idx, array in enumerate(C_tuple):\n","                if not isinstance(array, np.ndarray):\n","                    raise TypeError(f\"Element at C_factors_sequence[{idx}][{arr_idx}] is not a NumPy array.\")\n","                filename = os.path.join(save_dir, f'C_array_{idx}_{arr_idx}.npy')\n","                try:\n","                    np.save(filename, array)\n","                    print(f\"Saved {filename}\")\n","                except Exception as e:\n","                    print(f\"Failed to save {filename}: {e}\")\n","else:\n","    print(\"new_expr_signal is False; skipping save.\")"],"metadata":{"id":"2NFwQf-txmD_"},"id":"2NFwQf-txmD_","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## load factors"],"metadata":{"id":"6kTGr3ju-U82"},"id":"6kTGr3ju-U82"},{"cell_type":"code","source":["# Get all files in the directory\n","saved_files = os.listdir(save_dir)\n","\n","# Filter files for C and A arrays\n","C_factors_sequence_test = [f for f in saved_files if f.startswith('C_array_') and f.endswith('.npy')]\n","A_factors_sequence_test = [f for f in saved_files if f.startswith('A_array_') and f.endswith('.npy')]\n","\n","print(\"C_factors_sequence_test:\", C_factors_sequence_test)\n","print(\"A_factors_sequence_test:\", A_factors_sequence_test)"],"metadata":{"id":"vqafL8fa-Vu_"},"id":"vqafL8fa-Vu_","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## run `WDM`"],"metadata":{"id":"kNKvFkFJ0Ezb"},"id":"kNKvFkFJ0Ezb"},{"cell_type":"code","execution_count":null,"id":"aad79c1e-82cf-4700-b42c-05e11fc1d32e","metadata":{"id":"aad79c1e-82cf-4700-b42c-05e11fc1d32e"},"outputs":[],"source":["%%time\n","import importlib\n","import WassersteinDiffMap\n","importlib.reload(WassersteinDiffMap)\n","\n","\n","WDM = WassersteinDiffMap.WassersteinDifferentiationMapping(ranks, tau_in = tau_in, tau_out=tau_out, \\\n","                      gamma=gamma, max_iter=max_iter, min_iter=min_iter, device=device, dtype=torch.float64, \\\n","                     printCost=True, returnFull=False, alpha=alpha, beta=beta, \\\n","                      initialization='Full', init_args = None)\n","\n","WDM.gamma_smoothing(C_factors_sequence, A_factors_sequence)\n","\n","Qs_pred = [Q.cpu().numpy() for Q in WDM.Q_gammas]\n","Ts_pred = [T.cpu().numpy() for T in WDM.T_gammas]"]},{"cell_type":"code","source":["Ss = spatial_list\n","\n","node_labels1 = [None for Q in WDM.Q_gammas]\n","\n","clustering.plot_clusters_from_QT(Ss, Qs_pred, Ts_pred, node_labels1, dotsize=10)\n","clustering.diffmap_from_QT(Qs_pred, Ts_pred, cell_type_labels=node_labels1, dsf=.3)"],"metadata":{"id":"fnrqOIH5rTi0"},"id":"fnrqOIH5rTi0","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## save"],"metadata":{"id":"5ZfNhfFL3bR2"},"id":"5ZfNhfFL3bR2"},{"cell_type":"code","source":["for i,Q in enumerate(WDM.Q_gammas):\n","    np.save(filehandle_save + f'wdm_Q{i}.npy', Q.cpu().numpy())\n","\n","for i,T in enumerate(WDM.T_gammas):\n","    np.save(filehandle_save + f'wdm_T{i}{i+1}.npy', T.cpu().numpy())"],"metadata":{"id":"LhwfZdUr3cf5"},"id":"LhwfZdUr3cf5","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ari, ami"],"metadata":{"id":"8213-QN93NL0"},"id":"8213-QN93NL0"},{"cell_type":"code","source":["importlib.reload(util_zf)"],"metadata":{"id":"2p_9PrBXjmqr"},"id":"2p_9PrBXjmqr","execution_count":null,"outputs":[]},{"cell_type":"code","source":["gt_types_list = clustering.max_likelihood_clustering(Qs_gt)\n","pred_types_list = clustering.max_likelihood_clustering(Qs_pred)\n","\n","util_zf.compute_ARI_and_AMI(gt_types_list, pred_types_list)"],"metadata":{"id":"2-gCHBTzjVo2"},"id":"2-gCHBTzjVo2","execution_count":null,"outputs":[]},{"cell_type":"code","source":["importlib.reload(util_zf)"],"metadata":{"id":"gqFf6QiwA5eV"},"id":"gqFf6QiwA5eV","execution_count":null,"outputs":[]},{"cell_type":"code","source":["util_zf.silhouette(gt_types_list, pred_types_list, Xs, Ss)"],"metadata":{"id":"hd8zKG7HDu0d"},"id":"hd8zKG7HDu0d","execution_count":null,"outputs":[]},{"cell_type":"code","source":["util_zf.cos_silhouette(gt_types_list, pred_types_list, Xs, Ss)"],"metadata":{"id":"cPT51g7Zsqnq"},"id":"cPT51g7Zsqnq","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## freezing $Q$s with annotations, running `WDM`"],"metadata":{"id":"4pIn1OthuV98"},"id":"4pIn1OthuV98"},{"cell_type":"code","source":["%%time\n","Q_annot = Qs_gt_tensor\n","\n","importlib.reload(WassersteinDiffMap)\n","\n","WDM2 = WassersteinDiffMap.WassersteinDifferentiationMapping(ranks, tau_in = tau_in, tau_out=tau_out, \\\n","                      gamma=gamma, max_iter=100, min_iter=100, device=device, dtype=torch.float64, \\\n","                     printCost=True, returnFull=False, alpha=alpha, beta=beta, \\\n","                      initialization='Full', init_args=None)\n","\n","WDM2.impute_annotated_transitions(C_factors_sequence, A_factors_sequence, Q_annot)\n","T_gammas = WDM2.T_gammas"],"metadata":{"id":"RtExnEztuaUr"},"id":"RtExnEztuaUr","execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i,Q in enumerate(WDM2.Q_gammas):\n","    np.save(filehandle_save + f'ann_Q{i}.npy', Q.cpu().numpy())\n","\n","for i,T in enumerate(WDM2.T_gammas):\n","    np.save(filehandle_save + f'ann_T{i}{i+1}.npy', T.cpu().numpy())"],"metadata":{"id":"2EU8p6935NQg"},"id":"2EU8p6935NQg","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Analysis part 1: entropy of transitions"],"metadata":{"id":"2wcr26Iz6rN-"},"id":"2wcr26Iz6rN-"},{"cell_type":"code","source":["Ts_ann = T_gammas\n","Ts_pred = Ts_pred"],"metadata":{"id":"Voq_RKzh7WF3"},"id":"Voq_RKzh7WF3","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## entropy functions"],"metadata":{"id":"cq0cw-lj72HI"},"id":"cq0cw-lj72HI"},{"cell_type":"code","source":["importlib.reload(util_zf)"],"metadata":{"id":"1awKOzu173zQ"},"id":"1awKOzu173zQ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["util_zf.compare_T_entropies(Ts_ann, Ts_pred)\n","print('\\n')\n","util_zf.compare_T_col_entropies(Ts_ann, Ts_pred)"],"metadata":{"id":"6_BWPQvLyYcV"},"id":"6_BWPQvLyYcV","execution_count":null,"outputs":[]},{"cell_type":"code","source":["importlib.reload(util_zf)"],"metadata":{"id":"w4JTW9ivkBdd"},"id":"w4JTW9ivkBdd","execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i, (T_ann, T_pred) in enumerate(zip(Ts_ann, Ts_pred)):\n","    T_ann = T_ann.cpu().numpy()\n","    util_zf.plot_collision_profiles(T_ann, T_pred, title=f'Convergence profiles at slice {i+2}')"],"metadata":{"id":"dJdRCoG-kPEq"},"id":"dJdRCoG-kPEq","execution_count":null,"outputs":[]},{"cell_type":"code","source":["clustering.diffmap_from_QT(Qs_gt, Ts_ann, cell_type_labels=ct_labels, dsf=.3)"],"metadata":{"id":"si2xWwj_xEFf"},"id":"si2xWwj_xEFf","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## what about arg-secondmax? Does this have biological signal?"],"metadata":{"id":"6avxGdQ0CKKi"},"id":"6avxGdQ0CKKi"},{"cell_type":"code","source":["labels_2nd = []\n","\n","for Q in Qs_pred:\n","    # Get the indices of the sorted entries for each row\n","    sorted_indices = np.argsort(Q, axis=1)\n","\n","    # Extract the second-to-last index (the second largest) from the sorted indices\n","    second_largest_indices = sorted_indices[:, -2]\n","    labels_2nd += [second_largest_indices]"],"metadata":{"id":"s1iot__gCQpK"},"id":"s1iot__gCQpK","execution_count":null,"outputs":[]},{"cell_type":"code","source":["threshold = 0.1  # Example threshold as a percentage of the largest value\n","\n","labels_consensus = []\n","\n","for Q in Qs_pred:\n","    argmax_labels = np.argmax(Q, axis=1)\n","    sorted_indices = np.argsort(Q, axis=1)\n","    second_argmax_labels = sorted_indices[:, -2]\n","\n","    # Get the largest and second largest values from Q\n","    largest_values = np.max(Q, axis=1)\n","    second_largest_values = Q[np.arange(Q.shape[0]), second_argmax_labels]\n","\n","    # Apply the threshold to choose between argmax and second argmax\n","    consensus_labels = np.where((largest_values - second_largest_values) / largest_values < threshold,\n","                                second_argmax_labels, argmax_labels)\n","\n","    # Convert consensus_labels (NumPy array) to a regular Python list of ints\n","    labels = consensus_labels.tolist()  # Convert the array to a list\n","    labels = [int(label) for label in labels]  # Ensure all elements are ints\n","    labels = np.array(labels)\n","    labels_consensus.append(labels)  # Use append to add the new list of labels\n","\n","# Now labels_consensus is a list of lists, suitable for plotting\n","clustering.plot_clustering_list(Ss, labels_consensus, dotsize=10)"],"metadata":{"id":"gs94PVK4E3Mq"},"id":"gs94PVK4E3Mq","execution_count":null,"outputs":[]},{"cell_type":"code","source":["util_zf.compute_ARI_and_AMI(gt_types_list, labels_consensus)"],"metadata":{"id":"tuQnCsRTA82G"},"id":"tuQnCsRTA82G","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from scipy.special import softmax\n","\n","def sample_random_labeling(Q, T=1.0, epsilon=1e-12):\n","    # Step 1: Normalize Q row-wise to make it row-stochastic\n","    row_sums = Q.sum(axis=1, keepdims=True)\n","    P = Q / row_sums\n","\n","    # Handle division by zero in case row sums are zero\n","    P = np.nan_to_num(P)\n","\n","    # Step 2: Take the logarithm of each element (add epsilon to avoid log(0))\n","    L = np.log(P + epsilon)\n","\n","    # Step 3: Apply the built-in softmax function with temperature scaling\n","    # Adjust the input by dividing by the temperature T\n","    S = softmax(L / T, axis=1)\n","\n","    # Step 4: Sample a label for each row based on the probabilities in S\n","    num_rows, num_cols = S.shape\n","    random_labels = np.array([\n","        np.random.choice(num_cols, p=S[i])\n","        for i in range(num_rows)\n","    ])\n","\n","    return random_labels"],"metadata":{"id":"NZIr4J_-DKlQ"},"id":"NZIr4J_-DKlQ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import adjusted_mutual_info_score as ami\n","from sklearn.metrics import adjusted_rand_score as ari\n","\n","def get_ARI_and_AMI(gt_types_list, pred_types_list, x_percent=5):\n","    #print(f\"ARI and AMI of predictions (filtered excludes ground truth clusters smaller than {x_percent}% of the data)\\n\")\n","    aris = []\n","    amis = []\n","    for i, (gt_types, pred_types) in enumerate(zip(gt_types_list, pred_types_list)):\n","        raw_ari = ari(gt_types, pred_types)\n","        raw_ami = ami(gt_types, pred_types)\n","\n","        gt_labels = np.array(gt_types)\n","        pred_labels = np.array(pred_types)\n","        total_points = len(gt_labels)\n","\n","        # Compute counts of ground truth clusters\n","        unique_labels, counts = np.unique(gt_labels, return_counts=True)\n","        percentages = counts / total_points * 100\n","\n","        # Identify clusters to keep (clusters with size >= x%)\n","        clusters_to_keep = unique_labels[percentages >= x_percent]\n","\n","        # Create a mask to keep only data points in clusters_to_keep\n","        mask = np.isin(gt_labels, clusters_to_keep)\n","\n","        # Apply mask to both gt_labels and pred_labels\n","        gt_labels_filtered = gt_labels[mask]\n","        pred_labels_filtered = pred_labels[mask]\n","\n","        # Compute ARI and AMI on the filtered labels\n","        x_ari = ari(gt_labels_filtered, pred_labels_filtered)\n","        x_ami = ami(gt_labels_filtered, pred_labels_filtered)\n","\n","        aris += [x_ari]\n","        amis += [x_ami]\n","\n","    return aris, amis\n","\n","        #print(f'ARI for {i}th slice is {raw_ari:.3f} (filtered: {x_ari:.3f}) \\t')\n","        #print(f'AMI for {i}th slice is {raw_ami:.3f} (filtered: {x_ami:.3f})')\n","        #print('\\n')"],"metadata":{"id":"VTGawTfpLXtO"},"id":"VTGawTfpLXtO","execution_count":null,"outputs":[]},{"cell_type":"code","source":["N_samples = 50\n","\n","n_timepoints = len(Qs_pred)\n","\n","scores = np.zeros((N_samples, n_timepoints, 2))\n","\n","\n","#for i in range(N_samples):\n","\n","\n","'''\n","for i in range(N_samples):\n","\n","    labels_sampled = []\n","\n","    for t, Q in enumerate(Qs_pred):\n","        random_labels = sample_random_labeling(Q, T=1e-3)\n","        labels_sampled.append(random_labels)  # Use append to add the new list of labels\n","\n","    aris, amis = get_ARI_and_AMI(gt_types_list, labels_sampled)\n","    aris_inner_np = [ np.array(ari) for ari in aris ]\n","    amis_inner_np = [ np.array(ami) for ami in amis ]\n","\n","    aris_np = np.array(aris_inner_np)\n","    amis_np = np.array(amis_inner_np)\n","\n","    scores[i, :, 0] = aris_np\n","    scores[i, :, 1] = amis_np\n","''';"],"metadata":{"id":"BIgAAhvRJ97B"},"id":"BIgAAhvRJ97B","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(np.mean(scores[:,:,0], axis=0))"],"metadata":{"id":"aymn580zMb8c"},"id":"aymn580zMb8c","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(np.max(scores[:,:,0], axis=0))"],"metadata":{"id":"XyRiSsz0SJyg"},"id":"XyRiSsz0SJyg","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lwZlJwoGJ3cD"},"id":"lwZlJwoGJ3cD","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Analysis part 2: DE analysis"],"metadata":{"id":"HpXrTmOYuYT4"},"id":"HpXrTmOYuYT4"},{"cell_type":"markdown","source":["## load output of `WDM`"],"metadata":{"id":"p0gLQ6hMwGlb"},"id":"p0gLQ6hMwGlb"},{"cell_type":"code","source":["wdm_Qs = []\n","wdm_Ts = []\n","\n","\n","wdm_Q0 = np.load(filehandle_save + 'wdm_Q0.npy')\n","wdm_Q1 = np.load(filehandle_save + 'wdm_Q1.npy')\n","wdm_Q2 = np.load(filehandle_save + 'wdm_Q2.npy')\n","\n","wdm_T0 = np.load(filehandle_save + 'wdm_T0.npy')\n","wdm_T1 = np.load(filehandle_save + 'wdm_T1.npy')\n","\n","ann_Q0 = np.load(filehandle_save+ 'ann_Q0.npy')\n","\n","ann_Q1 = np.load(filehandle_save + 'ann_Q1.npy')\n","ann_Q1_imp = np.load(filehandle_save + 'ann_Q1_imp.npy')\n","\n","ann_Q2 = np.load(filehandle_save + 'ann_Q2.npy')\n","\n","ann_T0_imp = np.load(filehandle_save + 'ann_T0_imp.npy')\n","ann_T1_imp = np.load(filehandle_save + 'ann_T1_imp.npy')\n","\n","ann_T0 = np.load(filehandle_save + 'ann_T0.npy')\n","ann_T1 = np.load(filehandle_save + 'ann_T1.npy')\n","\n","### lists of these for the three cases\n","\n","Qs_ann = [ann_Q0, ann_Q1, ann_Q2]\n","Ts_ann = [ann_T0, ann_T1]\n","\n","Qs_ann_imp = [ann_Q0, ann_Q1_imp, ann_Q2]\n","Ts_ann_imp = [ann_T0_imp, ann_T1_imp]\n","\n","Qs_wdm = [wdm_Q0, wdm_Q1, wdm_Q2]\n","Ts_wdm = [wdm_T0, wdm_T1]"],"metadata":{"id":"qyLJMxyuuZSz"},"id":"qyLJMxyuuZSz","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## set ranks, set `i` = first timepoint of three: `i`, `i+1`, `i+2`"],"metadata":{"id":"FaXMFoaQweah"},"id":"FaXMFoaQweah"},{"cell_type":"code","source":["ranks = [(3,7),(7,7),(7,11),(11,14),(14,19)]\n","\n","i=2\n","\n","r1, r2 = ranks[i]\n","r2, r3 = ranks[i+1]"],"metadata":{"id":"7uTHSpQgwYB1"},"id":"7uTHSpQgwYB1","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## load features and spatial coords specific to the triple"],"metadata":{"id":"zrF1Rcr4w0i9"},"id":"zrF1Rcr4w0i9"},{"cell_type":"code","source":["s0_name = zf_names[i]\n","s1_name = zf_names[i+1]\n","s2_name = zf_names[i+2]\n","\n","filehandle_pair1 = filehandle_zf + 'pair' + str(i) + '/'\n","filehandle_pair2 = filehandle_zf + 'pair' + str(i+1) + '/'\n","\n","X0 = np.load(filehandle_pair1 + s0_name + '_feature.npy')\n","X1 = np.load(filehandle_pair1 + s1_name + '_feature.npy')\n","X2 = np.load(filehandle_pair2 + s2_name + '_feature.npy')\n","\n","S0 = spatial_list[i]\n","S1 = spatial_list[i+1]\n","S2 = spatial_list[i+2]\n","\n","Ss = [S0, S1, S2]"],"metadata":{"id":"OJ2L2d8QwxlB"},"id":"OJ2L2d8QwxlB","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## load labels"],"metadata":{"id":"CEvw_cwVw4M8"},"id":"CEvw_cwVw4M8"},{"cell_type":"code","source":["filehandle_ctd = 'drive/Othercomputers/numac/GitHub/celltypediscovery/'\n","\n","slice0_types = list(np.load(filehandle_ctd + 'a_gt_types/' + 'slice0_types.npy'))\n","slice1_types = list(np.load(filehandle_ctd + 'a_gt_types/' + 'slice1_types.npy'))\n","slice2_types = list(np.load(filehandle_ctd + 'a_gt_types/' + 'slice2_types.npy'))\n","slice3_types = list(np.load(filehandle_ctd + 'a_gt_types/' + 'slice3_types.npy'))\n","slice4_types = list(np.load(filehandle_ctd + 'a_gt_types/' + 'slice4_types.npy'))\n","slice5_types = list(np.load(filehandle_ctd + 'a_gt_types/' + 'slice5_types.npy'))\n","\n","ct_labels = [ list(set(slice0_types)),\n","               list(set(slice1_types)),\n","                list(set(slice2_types)),\n","                list(set(slice3_types)),\n","                list(set(slice4_types)),\n","                list(set(slice5_types))]\n","\n","target_value = 'Otic Vesicle'\n","# Find indices where the elements match the target value\n","indices = [index for index, value in enumerate(slice5_types) if value == 'Otic Vesicle']\n","\n","print(f\"Indices of '{target_value}' in the list: {indices}\")"],"metadata":{"id":"XHypdIyjw5AA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729118067671,"user_tz":240,"elapsed":195,"user":{"displayName":"Julian Gold","userId":"01597584131251118338"}},"outputId":"9fa18ab2-fd85-49b7-86bd-8a9dacecaacf"},"id":"XHypdIyjw5AA","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Indices of 'Otic Vesicle' in the list: [5041]\n"]}]},{"cell_type":"markdown","source":["## case 1: `ml` clustering,  annotations"],"metadata":{"id":"IyzH5iXQb2R-"},"id":"IyzH5iXQb2R-"},{"cell_type":"code","source":["type(slice5_types)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G3XQUZOidhM1","executionInfo":{"status":"ok","timestamp":1729117986159,"user_tz":240,"elapsed":204,"user":{"displayName":"Julian Gold","userId":"01597584131251118338"}},"outputId":"7113224a-ae18-481e-c7cd-1e621a9a1aff"},"id":"G3XQUZOidhM1","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["list"]},"metadata":{},"execution_count":241}]},{"cell_type":"code","source":["importlib.reload(clustering)\n","\n","\n","clustering.both_from_QT(Ss=Ss,\n","                        Qs=Qs_ann,\n","                        Ts=Ts_ann,\n","                        cell_type_labels=cell_type_labels_case_1,\n","                        clustering_type='ml')"],"metadata":{"id":"gdUsO5qgxCMx"},"id":"gdUsO5qgxCMx","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## case 2: `ml` clustering, annotations at $t_1, t_3$"],"metadata":{"id":"j4qA4qVbbm2_"},"id":"j4qA4qVbbm2_"},{"cell_type":"code","source":["importlib.reload(clustering)\n","\n","\n","clustering.both_from_QT(Ss=Ss,\n","                        Qs=Qs_ann_imp,\n","                        Ts=Ts_ann_imp,\n","                        cell_type_labels=cell_type_labels_case_2,\n","                        clustering_type='ml')"],"metadata":{"id":"IxiFOVMAeiKs"},"id":"IxiFOVMAeiKs","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## case 3: fully unsupervised"],"metadata":{"id":"FKjzutOwfOcZ"},"id":"FKjzutOwfOcZ"},{"cell_type":"code","source":["importlib.reload(clustering)\n","\n","\n","clustering.both_from_QT(Ss=Ss,\n","                        Qs=Qs_wdm,\n","                        Ts=Ts_wdm,\n","                        cell_type_labels=cell_type_labels_case_3,\n","                        clustering_type='ml')"],"metadata":{"id":"Bhd7qiyLfSd4"},"id":"Bhd7qiyLfSd4","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## focus on case 3, what does differential expression reveal about how these clusters compare to the annotations?"],"metadata":{"id":"oDGH8uNskxaZ"},"id":"oDGH8uNskxaZ"},{"cell_type":"code","source":["pred_clustering_list = clustering.max_likelihood_clustering(Qs_wdm)\n","gt_clustering_list = clustering.max_likelihood_clustering(Qs_gt[2:5])"],"metadata":{"id":"SW-ZoraFfJnO"},"id":"SW-ZoraFfJnO","execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred_clustering_0 = pred_clustering_list[0]\n","pred_clustering_1 = pred_clustering_list[1]\n","pred_clustering_2 = pred_clustering_list[2]"],"metadata":{"id":"C3GOZYil7SMd"},"id":"C3GOZYil7SMd","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## differential expression at 10hpf:"],"metadata":{"id":"c7St6PWmVrny"},"id":"c7St6PWmVrny"},{"cell_type":"code","source":["_adata0 = adatas[2].copy()\n","\n","_adata0.obs['diff_map_cluster'] = pred_clustering_0\n","_adata0.obs['diff_map_cluster'] = _adata0.obs['diff_map_cluster'].astype('category')\n","sc.pp.normalize_total(_adata0)\n","sc.pp.log1p(_adata0)\n","\n","# Differential gene expression analysis\n","sc.tl.rank_genes_groups(_adata0, groupby='diff_map_cluster', method='t-test', key_added = \"t-test\")\n","sc.pl.rank_genes_groups(_adata0, n_genes=10, sharey=False, key = \"t-test\")\n","\n","# Bin-annotation\n","sc.tl.rank_genes_groups(_adata0, groupby='bin_annotation', method='t-test', key_added = \"t-test\")\n","sc.pl.rank_genes_groups(_adata0, n_genes=10, sharey=False, key = \"t-test\")\n"],"metadata":{"id":"RxQ-cdHsPDeO"},"id":"RxQ-cdHsPDeO","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## most interesting predicted clusters at 10hpf"],"metadata":{"id":"RCQ1CzvMXWwJ"},"id":"RCQ1CzvMXWwJ"},{"cell_type":"markdown","source":["## differential expression at 12hpf:"],"metadata":{"id":"AzJ1y4_PV49X"},"id":"AzJ1y4_PV49X"},{"cell_type":"code","source":["_adata1 = adatas[3].copy()\n","\n","_adata1.obs['diff_map_cluster'] = pred_clustering_1\n","_adata1.obs['diff_map_cluster'] = _adata1.obs['diff_map_cluster'].astype('category')\n","sc.pp.normalize_total(_adata1)\n","sc.pp.log1p(_adata1)\n","\n","# Differential gene expression analysis\n","sc.tl.rank_genes_groups(_adata1, groupby='diff_map_cluster', method='t-test', key_added = \"t-test\")\n","sc.pl.rank_genes_groups(_adata1, n_genes=10, sharey=False, key = \"t-test\")\n","\n","# Bin-annotation\n","sc.tl.rank_genes_groups(_adata1, groupby='bin_annotation', method='t-test', key_added = \"t-test\")\n","sc.pl.rank_genes_groups(_adata1, n_genes=10, sharey=False, key = \"t-test\")\n"],"metadata":{"id":"doH2L8E5V5HU"},"id":"doH2L8E5V5HU","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## most interesting predicted clusters at 12hpf"],"metadata":{"id":"JtUVSVZ4XaiZ"},"id":"JtUVSVZ4XaiZ"},{"cell_type":"markdown","source":["## differential expression at 18hpf:"],"metadata":{"id":"fwz8zfbRWPhp"},"id":"fwz8zfbRWPhp"},{"cell_type":"code","source":["_adata2 = adatas[4].copy()\n","\n","_adata2.obs['diff_map_cluster'] = pred_clustering_2\n","_adata2.obs['diff_map_cluster'] = _adata2.obs['diff_map_cluster'].astype('category')\n","sc.pp.normalize_total(_adata2)\n","sc.pp.log1p(_adata2)\n","\n","# Differential gene expression analysis\n","sc.tl.rank_genes_groups(_adata2, groupby='diff_map_cluster', method='t-test', key_added = \"t-test\")\n","sc.pl.rank_genes_groups(_adata2, n_genes=10, sharey=False, key = \"t-test\")\n","\n","# Bin-annotation\n","sc.tl.rank_genes_groups(_adata2, groupby='bin_annotation', method='t-test', key_added = \"t-test\")\n","sc.pl.rank_genes_groups(_adata2, n_genes=10, sharey=False, key = \"t-test\")\n"],"metadata":{"id":"iYtIjk_mQIeT"},"id":"iYtIjk_mQIeT","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## most interesting predicted clusters at 18hpf"],"metadata":{"id":"7NfHBXuWW30W"},"id":"7NfHBXuWW30W"},{"cell_type":"code","source":[],"metadata":{"id":"LWC-WbMZXc-m"},"id":"LWC-WbMZXc-m","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"peterenv2 [~/.conda/envs/peterenv2/]","language":"python","name":"conda_peterenv2"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"colab":{"provenance":[],"collapsed_sections":["kNKvFkFJ0Ezb","5ZfNhfFL3bR2","8213-QN93NL0","4pIn1OthuV98","2wcr26Iz6rN-","zrF1Rcr4w0i9"],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}